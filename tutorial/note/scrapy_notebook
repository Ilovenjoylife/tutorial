#scrapy的爬虫示例：
爬虫的网址： http://quotes.toscrape.com
scrapy文档学习网址：   https://docs.scrapy.org/en/latest/intro/tutorial.html
scrapy中网学习网址：   http://scrapy-chs.readthedocs.io/zh_CN/0.24/intro/tutorial.html

安装指南
1、安装scrapy
用Anaconda或者Miniconda的方式安装，下面用conda安装scrapy
conda install -c conda-forge scrapy

用PyPi的方式安装
pip install Scrapy

2、需要知道的事情
Scrapy是用纯Python编写的，并且依赖于几个关键的Python包（等等）：

lxml，一种高效的XML和HTML解析器
parsel，一个写在lxml之上的HTML / XML数据提取库，
w3lib，一个用于处理URL和网页编码的多用途帮手
twisted，一个异步网络框架
cryptography和pyOpenSSL，来处理各种网络级别的安全需求

3、使用一个虚拟环境
虚拟环境的安装

$ [sudo] pip install virtualenv

4、特定平台的安装说明
Windows平台，尽管可以在Windows上使用pip安装Scrapy，但我们建议您安装Anaconda或Miniconda并使用conda - forge通道中的软件包 ，这样可以避免大部分安装问题。
安装Anaconda或Miniconda后，请使用以下命令安装Scrapy：

conda install -c conda-forge scrapy

Ubuntu 14.04 or 更早的版本
不建议使用Ubuntu自带的python-scrapy包，该包太旧了，无法赶上最新的scrapy包
安装依赖包

sudo apt-get install python-dev python-pip libxml2-dev libxslt1-dev zlib1g-dev libffi-dev libssl-dev

如果想让scrapy在python3上运行，你需要安装python3的开发头文件

sudo apt-get install python3 python3-dev

在virtualenv中，你可以pip在那之后安装Scrapy :

pip  install scrapy

注意
在Debian Jessie（8.0）及更高版本中，可以使用相同的非Python依赖项来安装Scrapy。

其它平台的安装方式，请参考网址：  https://doc.scrapy.org/en/latest/intro/install.html


Scrapy教程
在本教程中，我们假定Scrapy已经安装在您的系统上。如果不是这种情况，请参阅安装指南。

我们将去掉quotes.toscrape.com，这是一个列出着名作家引用的网站。

本教程将引导您完成这些任务：

1、创建一个新的Scrapy项目
2、编写蜘蛛抓取网站并提取数据
3、使用命令行导出刮取的数据
4、更改蜘蛛递归跟随链接
5、使用蜘蛛参数

1、创建一个项目
scrapy startproject tutorial   #tutorial 是项目名

下面是项目的目录结构
tutorial/
    scrapy.cfg            # 部署的配置文件

    tutorial/             # 项目的python模块，你的代码将导入到这里
        __init__.py

        items.py          # 项目列表的定义文件

        middlewares.py    # 项目中间件文件

        pipelines.py      # 项目中的pipelines文件

        settings.py       # 项目的设置文件

        spiders/          # 你将要使用的爬虫文件夹
            __init__.py



2、编写第一个爬虫文件
在tutorial/spider目录下创建一个python文件，文件名为quotes_spider.py ，文件的内如下：
import scrapy


class QuotesSpider(scrapy.Spider):
    name = "quotes"   

    def start_requests(self):
        urls = [
            'http://quotes.toscrape.com/page/1/',
            'http://quotes.toscrape.com/page/2/',
        ]
        for url in urls:
            yield scrapy.Request(url=url, callback=self.parse)

    def parse(self, response):
        page = response.url.split("/")[-2]
        filename = 'quotes-%s.html' % page
        with open(filename, 'wb') as f:
            f.write(response.body)
        self.log('Saved file %s' % filename)
        
代码说明：
name：爬虫的表示，它在全局中是唯一的,用于区分不同爬虫
start_requests（）:：必须返回Spider将开始抓取的请求的迭代（您可以返回一个列表的请求或写一个发生器函数）。随后的请求将从这些初始请求中连续生成。
parse()：将被调用来处理为每个请求下载的响应的方法。响应参数是TextResponse保存页面内容的一个实例，并有更多有用的方法来处理它。
该parse()方法通常解析响应，将提取的数据提取为字符串，并查找新的URL并Request根据它们创建新的请求（）。
urls: 请求的初始化的爬虫网站地址     

如何运行我们的蜘蛛
为了让我们的蜘蛛工作，请转到项目的顶层目录并运行：  

scrapy crawl quotes

start_requests方法的快捷方式

import scrapy


class QuotesSpider(scrapy.Spider):
    name = "quotes"
    start_urls = [
        'http://quotes.toscrape.com/page/1/',
        'http://quotes.toscrape.com/page/2/',
    ]

    def parse(self, response):
        page = response.url.split("/")[-2]
        filename = 'quotes-%s.html' % page
        with open(filename, 'wb') as f:
            f.write(response.body)

提取数据
学习如何使用Scrapy提取数据的最佳方法是尝试使用Shell Scrapy shell的选择器，运行：

scrapy shell 'http://quotes.toscrape.com/page/1/'

Windows平台，使用的是双引号：
scrapy shell "http://quotes.toscrape.com/page/1/"

使用这个shell，你可以用CSS的元素来请求这个对象：

>>> response.css('title')
[<Selector xpath=u'descendant-or-self::title' data=u'<title>Quotes to Scrape</title>'>]

运行的结果response.css('title')是一个名为的类似列表的对象 SelectorList，它表示一系列Selector围绕XML / HTML元素的对象列表， 并允许您运行更多查询来细化选择或提取数据。

从title中提取文本数据，你可以这么做：
>>> response.css('title::text').extract()
[u'Quotes to Scrape']

这里需要注意两点：其一是我们已经添加::text到CSS查询中，意思是我们只想选择元素内部的文本元素 <title>。如果我们没有指定::text，我们会得到完整的标题元素，包括它的标签：
>>> response.css('title').extract()
[u'<title>Quotes to Scrape</title>']

另一件事是调用的结果.extract()是一个列表，因为我们正在处理一个实例SelectorList。当你知道你只是想要第一个结果，就像在这种情况下，你可以这样做：
>>> response.css('title::text').extract_first()
u'Quotes to Scrape'

另一种方式，你可以这样写：
>>> response.css('title::text')[0].extract()
u'Quotes to Scrape'

但是，如果找不到与选择相匹配的元素，则使用.extract_first()避免IndexError并返回 None。

这里有一个教训：对于大多数抓取代码，您希望它能够灵活地处理由于在页面上未找到的东西而导致的错误，因此即使某些部分无法被抓取，您至少也可以获取一些数据。

除了extract()和 extract_first()方法之外，还可以使用该re()方法使用正则表达式进行提取：

>>> response.css('title::text').re(r'Quotes.*')
[u'Quotes to Scrape']
>>> response.css('title::text').re(r'Q\w+')
[u'Quotes']
>>> response.css('title::text').re(r'(\w+) to (\w+)')
[u'Quotes', u'Scrape']


为了找到合适的CSS选择器来使用，你可能会发现在你的web浏览器中用shell打开响应页面很有用view(response)。您可以使用浏览器开发工具或Firebug等扩展（请参阅关于使用Firebug进行抓取和使用Firefox进行抓取的部分）。
Selector Gadget也是一个很好的工具，可以快速找到可供选择的元素的CSS选择器，这可以在许多浏览器中使用。

XPath：一个简短的介绍
除CSS外，Scrapy选择器还支持使用XPath表达式：



XPath表达式非常强大，是Scrapy选择器的基础。实际上，CSS选择器在引擎盖下转换为XPath。您可以看到，如果仔细阅读shell中选择器对象的文本表示形式。

虽然可能不如CSS选择器那么受欢迎，但XPath表达式提供了更多的功能，因为除了浏览结构之外，它还可以查看内容。使用XPath，您可以选择如下内容：选择包含文本“下一页”的链接。这使得XPath非常适合抓取任务，并且即使您已经知道如何构建CSS选择器，我们也鼓励您学习XPath，这会使抓取更容易。

我们不会在这里介绍很多XPath，但您可以在这里阅读更多有关使用Scrapy选择器的XPath的信息。要了解关于XPath的更多信息，我们推荐本教程通过示例学习XPath，本教程将学习“如何用XPath思考”。

提取引号和作者
现在您已经了解了一些关于选择和提取的内容，让我们通过编写代码来从网页中提取引号来完成我们的蜘蛛。

http://quotes.toscrape.com中的每个引用都由HTML元素表示，如下所示：












